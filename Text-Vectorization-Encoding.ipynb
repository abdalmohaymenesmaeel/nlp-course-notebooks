{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59595119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Using cached torchtext-0.13.1-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from torchtext) (1.23.3)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp310-cp310-win_amd64.whl (162.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\python310\\lib\\site-packages (from torch==1.12.1->torchtext) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python310\\lib\\site-packages (from requests->torchtext) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->torchtext) (1.26.12)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.9.14-py3-none-any.whl (162 kB)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm->torchtext) (0.4.5)\n",
      "Installing collected packages: tqdm, torch, certifi, requests, torchtext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python310\\\\Scripts\\\\tqdm.exe' -> 'C:\\\\Python310\\\\Scripts\\\\tqdm.exe.deleteme'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f3aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421257f1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8bcc29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arsentd-lev',\n",
       " 'ArSenTD-LEV.tsv',\n",
       " 'arsentd-lev.zip',\n",
       " 'char_tokenizer.pkl',\n",
       " 'MNIST',\n",
       " 'word_tokenizer.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join(os.curdir, \"data\")\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab171d7c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b479f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Country</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Expression</th>\n",
       "      <th>Sentiment_Target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...</td>\n",
       "      <td>lebanon</td>\n",
       "      <td>personal</td>\n",
       "      <td>negative</td>\n",
       "      <td>implicit</td>\n",
       "      <td>بريق العيون</td>\n",
       "      <td>23</td>\n",
       "      <td>132</td>\n",
       "      <td>أؤمن بأن الانسان ينطفئ جماله ابتعاد يحب بريق ا...</td>\n",
       "      <td>اؤم بأن انس طفئ جمل بعد يحب برق عين خفي صبح ذب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...</td>\n",
       "      <td>jordan</td>\n",
       "      <td>sports</td>\n",
       "      <td>positive</td>\n",
       "      <td>explicit</td>\n",
       "      <td>افضل لاعب في العالم</td>\n",
       "      <td>23</td>\n",
       "      <td>141</td>\n",
       "      <td>الذاكره عندما اعتقد كريستيانو انه افضل لاعب ال...</td>\n",
       "      <td>ذكر عند عقد كريستيانو انه فضل لعب علم ككا يسي ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لا نخلو من ضغوطات الحياة. فنحن نعيش على أرض أع...</td>\n",
       "      <td>palestine</td>\n",
       "      <td>personal</td>\n",
       "      <td>neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>24</td>\n",
       "      <td>133</td>\n",
       "      <td>نخلو ضغوطات الحياة فنحن نعيش أرض أعدت للبلاء و...</td>\n",
       "      <td>خلو ضغط حية فنح نعش ارض اعد بلء ولم سلم بيء وك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...</td>\n",
       "      <td>lebanon</td>\n",
       "      <td>personal</td>\n",
       "      <td>negative</td>\n",
       "      <td>explicit</td>\n",
       "      <td>مصطلحات_لبنانيه</td>\n",
       "      <td>23</td>\n",
       "      <td>135</td>\n",
       "      <td>بتوصل عالبيت بنط بقلك جيت بتقعد لتتحدث معو بقل...</td>\n",
       "      <td>وصل علب بنط بقل جيت قعد حدث معو بقل شو تقم تمش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...</td>\n",
       "      <td>palestine</td>\n",
       "      <td>personal</td>\n",
       "      <td>negative</td>\n",
       "      <td>explicit</td>\n",
       "      <td>س لم تعد كما ك</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>نصمت لتسير حياتنا يرام فالناس تعد كآنت نقيه</td>\n",
       "      <td>نصم تسر حيت يرم لنس تعد كآن نقه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet    Country     Topic  \\\n",
       "0  \"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...    lebanon  personal   \n",
       "1  من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...     jordan    sports   \n",
       "2  لا نخلو من ضغوطات الحياة. فنحن نعيش على أرض أع...  palestine  personal   \n",
       "3  #مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...    lebanon  personal   \n",
       "4  نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...  palestine  personal   \n",
       "\n",
       "  Sentiment Sentiment_Expression     Sentiment_Target  word_count  char_count  \\\n",
       "0  negative             implicit          بريق العيون          23         132   \n",
       "1  positive             explicit  افضل لاعب في العالم          23         141   \n",
       "2   neutral                 none                 none          24         133   \n",
       "3  negative             explicit      مصطلحات_لبنانيه          23         135   \n",
       "4  negative             explicit       س لم تعد كما ك          16          67   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  أؤمن بأن الانسان ينطفئ جماله ابتعاد يحب بريق ا...   \n",
       "1  الذاكره عندما اعتقد كريستيانو انه افضل لاعب ال...   \n",
       "2  نخلو ضغوطات الحياة فنحن نعيش أرض أعدت للبلاء و...   \n",
       "3  بتوصل عالبيت بنط بقلك جيت بتقعد لتتحدث معو بقل...   \n",
       "4        نصمت لتسير حياتنا يرام فالناس تعد كآنت نقيه   \n",
       "\n",
       "                                       clean_stemmed  \n",
       "0  اؤم بأن انس طفئ جمل بعد يحب برق عين خفي صبح ذب...  \n",
       "1  ذكر عند عقد كريستيانو انه فضل لعب علم ككا يسي ...  \n",
       "2  خلو ضغط حية فنح نعش ارض اعد بلء ولم سلم بيء وك...  \n",
       "3  وصل علب بنط بقل جيت قعد حدث معو بقل شو تقم تمش...  \n",
       "4                    نصم تسر حيت يرم لنس تعد كآن نقه  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_filepath = os.path.join(data_dir, \"ArSenTD-LEV.tsv\")\n",
    "raw_dataset_url = \"https://shai-nlp-course.netlify.app/clean-tweets.tsv\"\n",
    "raw = pd.read_csv(filepath_or_buffer=raw_dataset_url, sep=\"\\t\")\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b10e9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe0fbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a generic Tokenizer class\n",
    "# Different tokenizers will inherit from this class\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus: list[str]):\n",
    "        self.vocab = self._create_vocab(corpus=corpus)\n",
    "        \n",
    "    def _create_vocab(self, corpus: list[str]) -> dict[str, int]:\n",
    "        ...\n",
    "    \n",
    "    def _tokenize_document(self, document: str) -> list[int]:\n",
    "        ...\n",
    "    \n",
    "    def tokenize(self, documents: list[str]) -> list[list[int]]:\n",
    "        return [self._tokenize_document(document) for document in documents]\n",
    "\n",
    "class WordLevelTokenizer(Tokenizer):\n",
    "    def __init__(self, corpus: list[str]):\n",
    "        super().__init__(corpus=corpus)\n",
    "        \n",
    "    def _create_vocab(self, corpus: list[str]) -> dict[str, int]:\n",
    "        all_word_tokens = set([token for sample in corpus for token in sample.split(\" \")])\n",
    "        vocab = {token: index for index, token in enumerate(word_level_tokens, start=1)} \n",
    "        return vocab\n",
    "    \n",
    "    def _tokenize_document(self, document: str) -> list[int]:\n",
    "        return [self.vocab.get(token, -1) for token in document.split(\" \")]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb47327",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = os.path.join(data_dir, \"word_tokenizer.pkl\")\n",
    "\n",
    "with open(tokenizer_path, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc4007",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c973d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw[\"clean_stemmed\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5bd3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = tokenizer.tokenize(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47171104",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(tokenized_tweets)\n",
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49062956",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c662e8",
   "metadata": {},
   "source": [
    "## One Hot (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11d4d4",
   "metadata": {},
   "source": [
    "Desired shape is: `[num_samples, vocab_size]`\n",
    "\n",
    "1. Define matrix of the desired shape, filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505d7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_onehot = np.zeros((num_samples, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36add2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sample in enumerate(tokenized_tweets):\n",
    "    for token in sample:\n",
    "        bow_onehot[index, token] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dcc13",
   "metadata": {},
   "source": [
    "## Count Encoding (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f003c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Count Encoding\n",
    "bow_count = np.zeros(()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce745a",
   "metadata": {},
   "source": [
    "## One Hot Encoding (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6205",
   "metadata": {},
   "source": [
    "Desired Shape: `[num_samples, max_length, vocab_size]`\n",
    "\n",
    "> Padding is required\n",
    "\n",
    "But first let's implement, creating a matrix for each document with shape: `[len_document, vocab_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8957f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_seq = []\n",
    "for sample in tokenized_tweets:\n",
    "    doc_matrix = np.zeros((len(sample), vocab_size))\n",
    "    for token_index, token in enumerate(sample):\n",
    "        doc_matrix[token_index, token] = 1\n",
    "    ohe_seq.append(doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d6c01",
   "metadata": {},
   "source": [
    "### Determine Max Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcd57d",
   "metadata": {},
   "source": [
    "In order to create a matrix of fixed dimensions, we need to set a fixed length for all sentences aliased as `max_length`\n",
    "\n",
    "There are two approaches to determine this value:\n",
    "1. Find the document with the maximum length\n",
    "2. Find the *nth* percentile (95th, 98th, ..etc) for the docuemnts' lengths\n",
    "\n",
    "The second approach is recommended, since it takes into account outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c4f4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs_lengths = [len(document) for document in tokenized_tweets]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "\n",
    "sns.boxplot(y=docs_lengths, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d46011",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = int(np.percentile(a=docs_lengths, q=98))\n",
    "pad_idx = tokenizer.vocab.get('[PAD]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254b218",
   "metadata": {},
   "source": [
    "One approach to creating padded sequences is:\n",
    "1. Create the output matrix as all padding\n",
    "2. Limit documents with length greater than `max_length`\n",
    "3. Replace the padding vectors with real token vectors whenever found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c809d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "ohe_seq = np.zeros((num_samples, max_length, vocab_size))\n",
    "ohe_seq[:, :, pad_idx] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fb0ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_index, document in enumerate(tokenized_tweets):\n",
    "    \n",
    "    # Step 2\n",
    "    document = document if len(document) <= max_length else document[0:max_length]\n",
    "    \n",
    "    # Step 3\n",
    "    for token_order, token in enumerate(document):\n",
    "        ohe_seq[doc_index, token_order, pad_idx] = 0\n",
    "        ohe_seq[doc_index, token_order, token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1dc66ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 21, 7755)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff6389",
   "metadata": {},
   "source": [
    "## One Hot Encoding Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d469dd1",
   "metadata": {},
   "source": [
    "Perform one hot encoding with a third party library\n",
    "\n",
    "Suggestions: \n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "- [torchtext](https://pytorch.org/text/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9598995",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ca495",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.fit(tokenized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc01d8-c53b-4826-b3e2-5a874f03d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterable\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "corpus = [\"The cat sat the mat\", \"The dog ate my homework\"]\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "voc = build_vocab_from_iterator(tokens)\n",
    "\n",
    "def my_one_hot(voc, keys: Union[str, Iterable]):\n",
    "    if isinstance(keys, str):\n",
    "        keys = [keys]\n",
    "    return F.one_hot(torch.tensor(voc(keys)), num_classes=len(voc))\n",
    "my_one_hot(tokens,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcdfda1",
   "metadata": {},
   "source": [
    "## TF-IDF (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111b08a",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. For the corpus calculate document frequency for each token (DF)\n",
    "2. For the corpus calculate inverse of document frequency for each token (IDF)\n",
    "3. For each document calculate term frequency (TF) for each token\n",
    "4. Calculate TF-IDF\n",
    "\n",
    "Desired output shape: `[num_samples, vocab_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31331296-546b-4fc8-a1d0-585ddc620c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e11098-d15d-48dd-b95f-80d7b9dac326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ae66a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required module\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize \n",
    "\n",
    "class TfIdfVectorizer:\n",
    "    def __init__(self,corpus):\n",
    "        self.corpus=corpus\n",
    "        self.vectors=[]\n",
    "        self.sentences=[]\n",
    "        self.word_set=[]\n",
    "        self.total_documents = len(self.corpus)\n",
    "        self.tokeniz()\n",
    "        self.index_dict = {}\n",
    "        self.word_set = set(self.word_set )\n",
    "        self.create_vocab()\n",
    "    def tokeniz(self):\n",
    "        for sent in self.corpus:\n",
    "            print(sent)\n",
    "            x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "            self.sentences.append(x)\n",
    "            for word in x:\n",
    "                if word not in self.word_set:\n",
    "                    self.word_set.append(word)\n",
    "    def create_vocab(self):\n",
    "        #Creating an index for each word in our vocab.\n",
    "        i = 0\n",
    "        for word in self.word_set:\n",
    "            self.index_dict[word] = i\n",
    "            i += 1 \n",
    "    def count_dict(sentences):\n",
    "        word_count = {}\n",
    "        for word in self.word_set:\n",
    "            word_count[word] = 0\n",
    "            for sent in sentences:\n",
    "                print(\"--\",end=\"\")\n",
    "                print(sent)\n",
    "                if word in sent:\n",
    "                    word_count[word] += 1\n",
    "        return word_count\n",
    "\n",
    "    #Term Frequency\n",
    "    def termfreq(self,document, word):\n",
    "        N = len(document)\n",
    "        occurance = len([token for token in document if token == word])\n",
    "        return occurance/N\n",
    "    #Inverse Document Frequency\n",
    "    def inverse_doc_freq(self,word):\n",
    "        try:\n",
    "            word_occurance = word_count[word] + 1\n",
    "        except:\n",
    "            word_occurance = 1\n",
    "        return np.log(self.total_documents/word_occurance)\n",
    "    \n",
    "    def tf_idf(self,sentence):\n",
    "        print(sentence)\n",
    "        tf_idf_vec = np.zeros((len(self.word_set),))\n",
    "        for word in sentence:\n",
    "            tf = self.termfreq(sentence,word)\n",
    "            idf = self.inverse_doc_freq(word)\n",
    "            value = tf*idf\n",
    "            tf_idf_vec[self.index_dict[word]] = value \n",
    "        return tf_idf_vec\n",
    "    def fit(self):\n",
    "        #TF-IDF Encoded text corpus\n",
    "        for sent in self.sentences:\n",
    "            print(sent)\n",
    "            vec = self.tf_idf(sent)\n",
    "            self.vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4a31320-4b0f-4b81-9295-96ae371bc5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[\n",
    "\"Welcome in our session \",\n",
    "    \"this session is about programming in our session \",\n",
    "    \"my pc is good \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8910f697-7f00-44e1-ad4d-7645339859d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in our session \n",
      "this session is about programming in our session \n",
      "my pc is good \n"
     ]
    }
   ],
   "source": [
    "objtfidf=TfIdfVectorizer(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0521e7d1-339f-402c-a339-111cb0d4d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'in', 'our', 'session']\n",
      "['welcome', 'in', 'our', 'session']\n",
      "['this', 'session', 'is', 'about', 'programming', 'in', 'our', 'session']\n",
      "['this', 'session', 'is', 'about', 'programming', 'in', 'our', 'session']\n",
      "['my', 'pc', 'is', 'good']\n",
      "['my', 'pc', 'is', 'good']\n"
     ]
    }
   ],
   "source": [
    "objtfidf.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24649531-44e6-4747-9142-89b51713585e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a4dbf-a469-4b87-b916-4af0c1088371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
